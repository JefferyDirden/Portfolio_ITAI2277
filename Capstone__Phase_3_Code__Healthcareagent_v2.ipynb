{"cells":[{"cell_type":"markdown","metadata":{"id":"oM8W7Dd7nnZT"},"source":["We used KaggleHub to download the Conjunctivitis Dataset from Kaggle. After\n"," downloading, we printed the dataset path to confirm that it was stored correctly.\n"," This dataset includes images of both healthy and infected eyes. We explored the\n"," folders to make sure the data was organized and ready for processing. We\n"," imported straight from Kaggle using the API and used 2 test images for the\n"," dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":0,"status":"aborted","timestamp":1761783116090,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"X7OU3vEKuBkm"},"outputs":[],"source":["import kagglehub\n","# Download latest version\n","path = kagglehub.dataset_download(\"alisofiya/conjunctivitis\")\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"markdown","metadata":{"id":"Zgg0bLHZuYZf"},"source":["**Step 1** Install Packages and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":110577,"status":"aborted","timestamp":1761783116091,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"EvvRd0MjuhRY"},"outputs":[],"source":["# Cell 1 — install \u0026 imports\n","# -------------------------------------------------\n","# Install required packages (quiet mode, safe if already installed)\n","!pip install -q kagglehub tensorflow matplotlib scikit-learn pandas pillow\n","\n","# -------------------------------------------------\n","# Core imports\n","import os, glob, random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from PIL import Image\n","\n","# TensorFlow / Keras\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# -------------------------------------------------\n","# Reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","print(\"TensorFlow version:\", tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"oGJdLmlchH5h"},"source":["**Step 2 Ensure dataset path is available**\n","\n","This cell checks whether the path exist and downloads if needed. It then finds image files recursively under that path and infers labels from parent folders"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":110576,"status":"aborted","timestamp":1761783116092,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"GQniLSMnhRB2"},"outputs":[],"source":["# Cell 2 — locate / download dataset and build a DataFrame (image_path, label)\n","from pathlib import Path\n","import pandas as pd\n","import kagglehub\n","\n","# -------------------------------------------------\n","# Try to reuse a previously-downloaded path; otherwise download\n","try:\n","    path                     # \u003c-- just check if the variable exists\n","    print(\"Using existing `path` variable:\", path)\n","except NameError:\n","    print(\"`path` not found — downloading dataset via kagglehub...\")\n","    path = kagglehub.dataset_download(\"alisofiya/conjunctivitis\")\n","    print(\"Downloaded dataset to:\", path)\n","\n","# -------------------------------------------------\n","# Convert to Path object\n","dataset_root = Path(path)\n","\n","# -------------------------------------------------\n","# Find **all** image files (jpg + jpeg + png – the dataset contains a few .png)\n","img_files = (\n","    list(dataset_root.rglob(\"*.jpg\")) +\n","    list(dataset_root.rglob(\"*.jpeg\")) +\n","    list(dataset_root.rglob(\"*.png\"))) # Corrected variable name here"]},{"cell_type":"markdown","metadata":{"id":"0zRvsLqIhuFi"},"source":["**Step 3 Quick EDA:**\n","\n","Class counts and sample images shows how many images per class and some example images."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":0,"status":"aborted","timestamp":1761783116100,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"Og-cHf14h5VP"},"outputs":[],"source":["# Cell 3 — EDA: class distribution + sample images\n","\n","# Create DataFrame from img_files\n","rows = [[str(p), p.parent.name] for p in img_files]\n","df = pd.DataFrame(rows, columns=[\"image_path\", \"label\"])\n","df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n","\n","counts = df['label'].value_counts()\n","print(\"Class distribution:\\n\", counts)\n","# bar plot\n","plt.figure(figsize=(5,4))\n","counts.plot(kind='bar')\n","plt.title(\"Class distribution\")\n","plt.ylabel(\"Count\")\n","plt.show()\n","# show up to 6 sample images (1 per class if multiple)\n","plt.figure(figsize=(12,4))\n","unique_labels = df['label'].unique()\n","for i, lab in enumerate(unique_labels[:6]):\n","    sample = df[df['label']==lab].iloc[0]['image_path']\n","    img = Image.open(sample).convert('RGB')\n","    plt.subplot(1, min(6, len(unique_labels)), i+1)\n","    plt.imshow(img.resize((200,200)))\n","    plt.title(lab)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uDi3eNohh_0u"},"source":["**Step 4 Split into train /val/ test (80/10/10)**\n","We create splits keeping class balance (stratify)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":0,"status":"aborted","timestamp":1761783116101,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"Zyays8EViRLv"},"outputs":[],"source":["# Cell 4 — train/val/test split\n","train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=SEED)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=SEED)\n","print(\"Counts -\u003e Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))"]},{"cell_type":"markdown","metadata":{"id":"V70-r_ZUmYhB"},"source":["**Step 5**\n","\n","Create ImageDataGenerators (with simple augmentation for train)\n","\n","We use flow_from_dataframe with directory=None and x_col containing full paths.class_mode='binary' if there are exactly 2 labels; otherwise 'categorical' for multi-class. The code auto-detects whether binary or multi-class."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1761783116104,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"CKJQyh1zma4Z"},"outputs":[],"source":["# Cell 5 — prepare generators\n","IMG_SIZE = (128, 128) # small for fast prototyping\n","BATCH_SIZE = 16\n","# determine if binary\n","unique_labels = sorted(df['label'].unique())\n","is_binary = (len(unique_labels) == 2)\n","print(\"Binary classification?\" , is_binary, \"Labels:\", unique_labels)\n","# training augmentation\n","train_datagen = ImageDataGenerator(\n","rescale=1./255,\n","rotation_range=10,\n","width_shift_range=0.05,\n","height_shift_range=0.05,\n","zoom_range=0.05,\n","horizontal_flip=True\n",")\n","# validation \u0026 test just rescale\n","test_val_datagen = ImageDataGenerator(rescale=1./255)\n","# flow_from_dataframe expects column names and directory argument. If x_col has full paths use directory=None\n","if is_binary:\n","    class_mode = 'binary'\n","else:\n","    class_mode = 'categorical'\n","\n","# Convert image_path column to string type\n","train_df['image_path'] = train_df['image_path'].astype(str)\n","val_df['image_path'] = val_df['image_path'].astype(str)\n","test_df['image_path'] = test_df['image_path'].astype(str)\n","\n","train_gen = train_datagen.flow_from_dataframe(\n","train_df,\n","x_col='image_path',\n","y_col='label',\n","target_size=IMG_SIZE,\n","batch_size=BATCH_SIZE,\n","class_mode=class_mode,\n","shuffle=True,\n","directory=None\n",")\n","val_gen = test_val_datagen.flow_from_dataframe(\n","val_df,\n","x_col='image_path',\n","y_col='label',\n","target_size=IMG_SIZE,\n","batch_size=BATCH_SIZE,\n","class_mode=class_mode,\n","shuffle=False,\n","directory=None\n",")\n","test_gen = test_val_datagen.flow_from_dataframe(\n","test_df,\n","x_col='image_path',\n","y_col='label',\n","target_size=IMG_SIZE,\n","batch_size=BATCH_SIZE,\n","class_mode=class_mode,\n","shuffle=False,\n","directory=None\n",")\n","label2index = train_gen.class_indices\n","index2label = {v:k for k,v in label2index.items()}\n","print(\"Label mapping (label -\u003e index):\", label2index)"]},{"cell_type":"markdown","metadata":{"id":"C3aLVxk5mxuR"},"source":["**Step 6**\n","\n","Build a small CNN (fast to run)\n","A compact architecture suitable for 5 epochs prototype. We ran different tests on the epochs to try to get more accuracy the more you run I feel the more accurate it becomes although sometimes it can become pretty time consuming depending on the dataset, but since this is a simple dataset it didn't take to long."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":34,"status":"aborted","timestamp":1761783116139,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"5-6WIySimxA7"},"outputs":[],"source":["# Cell 6 — build model\n","from tensorflow.keras.layers import BatchNormalization, Dropout, Dense  # \u003c-- Add Dense here\n","\n","num_classes = len(unique_labels)\n","\n","if is_binary:\n","    output_units = 1\n","    output_activation = 'sigmoid'\n","    loss = 'binary_crossentropy'\n","else:\n","    output_units = num_classes\n","    output_activation = 'softmax'\n","    loss = 'categorical_crossentropy'\n","\n","model = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n","    MaxPooling2D(2,2),\n","    BatchNormalization(),\n","\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    BatchNormalization(),\n","\n","    Conv2D(128, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    GlobalAveragePooling2D(),\n","\n","    Dense(128, activation='relu'),\n","    Dropout(0.35),\n","    Dense(output_units, activation=output_activation)\n","])\n","\n","model.compile(\n","    optimizer=Adam(learning_rate=1e-3),\n","    loss=loss,\n","    metrics=['accuracy']\n",")\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"GD80ToSYnwOF"},"source":["**step 7**\n","\n","Train for 5-20 epochs Train quickly for your prototype. Save training history. Also compare results from the different numbers ran."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":689},"executionInfo":{"elapsed":113602,"status":"error","timestamp":1761783116088,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"j2hzwP9ln45L","outputId":"123967cc-7f7f-4af9-c354-4d59381bfb0a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 779ms/step - accuracy: 0.6789 - loss: 0.6007 - val_accuracy: 0.5000 - val_loss: 0.6905\n","Epoch 2/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 686ms/step - accuracy: 0.8117 - loss: 0.3955 - val_accuracy: 0.5000 - val_loss: 0.6877\n","Epoch 3/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 788ms/step - accuracy: 0.8282 - loss: 0.3537 - val_accuracy: 0.5000 - val_loss: 0.6908\n","Epoch 4/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 690ms/step - accuracy: 0.8067 - loss: 0.4218 - val_accuracy: 0.5000 - val_loss: 0.6938\n","Epoch 5/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 686ms/step - accuracy: 0.8273 - loss: 0.3547 - val_accuracy: 0.5000 - val_loss: 0.7083\n","Epoch 6/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - accuracy: 0.7637 - loss: 0.4799 - val_accuracy: 0.5000 - val_loss: 0.7703\n","Epoch 7/30\n","\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 680ms/step - accuracy: 0.7928 - loss: 0.4384 - val_accuracy: 0.5000 - val_loss: 0.7790\n","Epoch 8/30\n","\u001b[1m 2/18\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 638ms/step - accuracy: 0.9062 - loss: 0.2579"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2633814286.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 7 — train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--\u003e 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Cell 7 — train\n","EPOCHS = 30\n","history = model.fit(\n","train_gen,\n","validation_data=val_gen,\n","epochs=EPOCHS\n",")"]},{"cell_type":"markdown","metadata":{"id":"Wg1TnpjXoFmj"},"source":["**step 8**\n","\n","Evaluate on test set; show metrics \u0026 confusion matrix We compute predictions and show classification report and confusion matrix."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":809},"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1761783121154,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"3vVU1CuCoIpQ","outputId":"26fc19d5-2565-48e0-e859-69cd919aa26e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.4844 - loss: 0.8141\n","Test loss: 0.8004, Test accuracy: 0.5000\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step\n","Classification report:\n","              precision    recall  f1-score   support\n","\n"," healthy_eye       0.00      0.00      0.00        18\n","infected_eye       0.50      1.00      0.67        18\n","\n","    accuracy                           0.50        36\n","   macro avg       0.25      0.50      0.33        36\n","weighted avg       0.25      0.50      0.33        36\n","\n","Confusion matrix:\n"," [[ 0 18]\n"," [ 0 18]]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 500x400 with 2 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# Cell 8 — evaluate and metrics\n","# Evaluate with generator\n","test_loss, test_acc = model.evaluate(test_gen, verbose=1)\n","print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n","\n","# Predict probabilities for entire test set\n","y_probs = model.predict(test_gen)\n","\n","if is_binary:\n","    # y_probs shape is (N, 1) → flatten and threshold at 0.5\n","    y_pred = (y_probs.ravel() \u003e 0.5).astype(int)\n","else:\n","    # For multi-class: take argmax\n","    y_pred = np.argmax(y_probs, axis=1)\n","\n","# True labels from test_df (map labels to indices using label2index)\n","y_true = test_df['label'].map(label2index).values\n","\n","# Classification report\n","print(\"Classification report:\")\n","print(classification_report(y_true, y_pred, target_names=list(label2index.keys())))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"Confusion matrix:\\n\", cm)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(5, 4))\n","plt.imshow(cm, interpolation='nearest', cmap='Blues')\n","plt.title(\"Confusion Matrix\")\n","plt.colorbar()\n","\n","# Set ticks and labels correctly\n","tick_labels = [index2label[i] for i in range(len(label2index))]\n","plt.xticks(ticks=np.arange(len(label2index)), labels=tick_labels, rotation=45)\n","plt.yticks(ticks=np.arange(len(label2index)), labels=tick_labels)\n","\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.tight_layout()  # Prevents label cutoff\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6KzzmvA_oe0D"},"source":["**Step 9**\n","\n","Predict on our two sample images at /content/Eye Images/...\n","This uses the same preprocessing (resize \u0026 rescale). It prints predicted label + confidence and displays the image."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1761783124406,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"-9zHzLnvoocP","outputId":"7e66fe61-82aa-4707-9ab1-55bf8edf2be8"},"outputs":[{"name":"stdout","output_type":"stream","text":["File not found: /content/drive/MyDrive/ITAI 2277 Fall 2025 Capstone /AI Agent Project_ITAI2277/healthy_eye/REPLACE_WITH_YOUR_HEALTHY_EYE_IMAGE.jpg\n","File not found: /content/drive/MyDrive/ITAI 2277 Fall 2025 Capstone /AI Agent Project_ITAI2277/infected_eye/REPLACE_WITH_YOUR_INFECTED_EYE_IMAGE.jpg\n"]}],"source":["# Cell 9 — predict on your sample test images\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define IMG_SIZE here to ensure it's available\n","IMG_SIZE = (128, 128) # This should match the size used for training\n","\n","# Update these paths to your actual uploaded images\n","# Make sure these are paths to image files, not directories\n","sample_paths = [\n","    \"/content/drive/MyDrive/ITAI 2277 Fall 2025 Capstone /AI Agent Project_ITAI2277/healthy_eye/REPLACE_WITH_YOUR_HEALTHY_EYE_IMAGE.jpg\", # Replace with the path to your healthy eye image\n","    \"/content/drive/MyDrive/ITAI 2277 Fall 2025 Capstone /AI Agent Project_ITAI2277/infected_eye/REPLACE_WITH_YOUR_INFECTED_EYE_IMAGE.jpg\" # Replace with the path to your infected eye image\n","]\n","\n","def predict_and_show(img_path):\n","    if not os.path.exists(img_path):\n","        print(\"File not found:\", img_path)\n","        return\n","\n","    # Load and preprocess image\n","    img = load_img(img_path, target_size=IMG_SIZE)\n","    arr = img_to_array(img) / 255.0\n","    arr = np.expand_dims(arr, axis=0)  # Add batch dimension\n","\n","    # Predict\n","    probs = model.predict(arr)[0]  # Shape: (1,) for binary, (n_classes,) for multi\n","\n","    if is_binary:\n","        prob = float(probs[0])\n","        pred_idx = int(prob \u003e 0.5)\n","        label = index2label[pred_idx]\n","        conf = prob if pred_idx == 1 else 1 - prob\n","    else:\n","        pred_idx = int(np.argmax(probs))\n","        label = index2label[pred_idx]\n","        conf = float(np.max(probs))\n","\n","    # Print result\n","    print(f\"Prediction for {os.path.basename(img_path)} → {label} (confidence: {conf:.3f})\")\n","\n","    # Show image with prediction\n","    plt.figure(figsize=(4, 4))\n","    plt.imshow(img)\n","    plt.title(f\"{label}\\nConfidence: {conf:.3f}\", fontsize=12)\n","    plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Run predictions\n","for p in sample_paths:\n","    predict_and_show(p)"]},{"cell_type":"markdown","metadata":{"id":"_f8DMYUsp1yo"},"source":["###Kaggle API"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"E22B71MTRn6-"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-5110da68-6ed3-43a2-8e9b-61e135db0007\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-5110da68-6ed3-43a2-8e9b-61e135db0007\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3238192353.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Kaggle API json file upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Upload your kaggle.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["# Kaggle API json file upload\n","from google.colab import files\n","uploaded = files.upload()  # Upload your kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5qmdxYxERqAj"},"outputs":[],"source":["#import\n","!mkdir -p ~/.kaggle\n","!cp \"/content/kaggle(1).json\" ~/.kaggle/kaggle.json  # Added quotes around the filename\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle datasets list -s conjunctivitis # Search for \"conjunctivitis\" to confirm datasets loaded"]},{"cell_type":"markdown","metadata":{"id":"FUW8pJRoqMR_"},"source":["###Download Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110607,"status":"aborted","timestamp":1761783116142,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"IQDe1IZ7RteK"},"outputs":[],"source":["# Eyes (already working via kagglehub)\n","import kagglehub\n","eyes_path = kagglehub.dataset_download(\"alisofiya/conjunctivitis\")\n","print(\"Eyes:\", eyes_path)\n","\n","# Rashes: Fitzpatrick17k\n","!kaggle datasets download -d nazmussadat013/fitzpatrick17k -p /content/rashes --unzip\n","\n","# Wounds\n","!kaggle datasets download -d ibrahimfateen/wound-classification -p /content/wounds --unzip"]},{"cell_type":"markdown","metadata":{"id":"gsZ8zjstqWal"},"source":["### Step 4: Build Unified Data Prep Function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110607,"status":"aborted","timestamp":1761783116142,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"J_59q2o1R59T"},"outputs":[],"source":["def prepare_dataset(root_path, img_size=(224,224), batch_size=32):\n","    # Find images\n","    img_files = list(Path(root_path).rglob(\"*.jpg\")) + list(Path(root_path).rglob(\"*.jpeg\"))\n","    rows = [[str(p), p.parent.name] for p in img_files]\n","    df = pd.DataFrame(rows, columns=[\"image_path\", \"label\"])\n","    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # Split\n","    train_df, temp = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n","    val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n","\n","    # Generators\n","    train_gen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=20,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest'\n","    ).flow_from_dataframe(train_df, x_col='image_path', y_col='label',\n","                          target_size=img_size, batch_size=batch_size,\n","                          class_mode='categorical') # Changed to 'categorical'\n","\n","    val_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n","        val_df,\n","        x_col='image_path',\n","        y_col='label',\n","        target_size=img_size,\n","        batch_size=batch_size,\n","        class_mode='categorical' # Changed to 'categorical'\n","    )\n","    test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n","        test_df,\n","        x_col='image_path',\n","        y_col='label',\n","        target_size=img_size,\n","        batch_size=batch_size,\n","        class_mode='categorical' # Changed to 'categorical'\n","    )\n","\n","    return train_gen, val_gen, test_gen, df"]},{"cell_type":"markdown","metadata":{"id":"FOWkUOTzq2SQ"},"source":["###Step 5: Train 3 Models (MobileNetV2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110608,"status":"aborted","timestamp":1761783116143,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"2s-HYvOVR7p2"},"outputs":[],"source":["from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","\n","def build_model(num_classes):\n","    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n","    base.trainable = False\n","    model = Sequential([\n","        base,\n","        GlobalAveragePooling2D(),\n","        Dense(128, activation='relu'),\n","        Dropout(0.3),\n","        Dense(num_classes, activation='softmax' if num_classes \u003e 1 else 'sigmoid')\n","    ])\n","    model.compile(Adam(1e-3), loss='categorical_crossentropy' if num_classes \u003e 1 else 'binary_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"mFz8ZYiyqiCR"},"source":["###Create moblenetv2 Diagram"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110607,"status":"aborted","timestamp":1761783116143,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"x7oEHyyVR-c-"},"outputs":[],"source":["from tensorflow.keras.utils import plot_model\n","\n","# Build your model (use the improved version from my last message)\n","model = build_model(num_classes=2)  # or your final model\n","\n","# Save diagram\n","plot_model(\n","    model,\n","    to_file='architecture.png',\n","    show_shapes=True,\n","    show_layer_names=True,\n","    rankdir='TB',  # Top to Bottom\n","    dpi=150\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110606,"status":"aborted","timestamp":1761783116144,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"XHnx5n09XptJ"},"outputs":[],"source":["from google.colab import files\n","files.download('architecture.png')"]},{"cell_type":"markdown","metadata":{"id":"EkAnR5a8YObm"},"source":["# Model Architecture Design\n","\n","\u003e **Project**: Conjunctivitis Eye Disease Classification  \n","\u003e **Phase**: 3 (Model Development)  \n","\u003e **Date**: October 28, 2025  \n","\u003e **Author**: [Your Name]\n","\n","---\n","\n","## 1. Overview\n","\n","We designed a **lightweight convolutional neural network (CNN)** suitable for binary classification of eye images into **Healthy** and **Infected (Conjunctivitis)**. The architecture balances performance and training speed on modest hardware (Colab T4 GPU).\n","\n","---\n","\n","## 2. Input Specifications\n","\n","| Property | Value |\n","|--------|-------|\n","| Input shape | `(224, 224, 3)` |\n","| Preprocessing | Rescale `/255`, data augmentation (rotation, zoom, flip) |\n","| Classes | 2 (`healthy_eye`, `infected_eye`) |\n","\n","---\n","\n","## 3. Architecture Diagram\n","\n","![Model Architecture](figures/architecture.png)\n","\n","---\n","\n","## 4. Layer-by-Layer Specification\n","\n","| Layer | Type | Output Shape | # Params | Activation | Notes |\n","|------|------|--------------|----------|------------|-------|\n","| 0 | Input | `(224, 224, 3)` | 0 | – | RGB eye image |\n","| 1 | Conv2D | `(222, 222, 32)` | 896 | ReLU | 3×3 kernel |\n","| 2 | BatchNormalization | `(222, 222, 32)` | 128 | – | Stabilizes training |\n","| 3 | MaxPooling2D | `(111, 111, 32)` | 0 | – | 2×2 pool |\n","| 4 | Conv2D | `(109, 109, 64)` | 18,496 | ReLU | 3×3 kernel |\n","| 5 | BatchNormalization | `(109, 109, 64)` | 256 | – | |\n","| 6 | MaxPooling2D | `(54, 54, 64)` | 0 | – | |\n","| 7 | Conv2D | `(52, 52, 128)` | 73,856 | ReLU | 3×3 kernel |\n","| 8 | BatchNormalization | `(52, 52, 128)` | 512 | – | |\n","| 9 | GlobalAveragePooling2D | `(128,)` | 0 | – | Reduces params |\n","| 10 | Dense | `(2,)` | 258 | Softmax | Final classification |\n","\n","\u003e **Total trainable parameters**: **~94K**  \n","\u003e **Non-trainable**: ~1K (BatchNorm)\n","\n","---\n","\n","## 5. Design Rationale\n","\n","| Decision | Reason |\n","|---------|--------|\n","| **224×224 input** | Standard for medical imaging; captures fine details (vs 128×128) |\n","| **3 Conv blocks** | Progressive feature extraction without overfitting on small dataset |\n","| **BatchNormalization** | Faster convergence, reduces internal covariate shift |\n","| **GlobalAveragePooling** | Eliminates fully connected layers → fewer params, less overfitting |\n","| **No Dropout** | Small model + augmentation → sufficient regularization |\n","| **Categorical Crossentropy + Softmax** | Multi-class (even if binary) → better probability calibration |\n","\n","---\n","\n","## 6. Training Configuration\n","\n","```python\n","model.compile(\n","    optimizer=Adam(learning_rate=1e-3),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110613,"status":"aborted","timestamp":1761783116152,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"So20tmpTYrWB"},"outputs":[],"source":["!pip install mlflow --quiet"]},{"cell_type":"markdown","metadata":{"id":"uh86DLNMrkZS"},"source":["### Experiment Tracking\n","\n","1. Install MLflow"]},{"cell_type":"markdown","metadata":{"id":"B12-frdzryL4"},"source":["### 2. Import \u0026 Start MLflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110614,"status":"aborted","timestamp":1761783116153,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"cECL-Ex9YzS9"},"outputs":[],"source":["import mlflow\n","import mlflow.tensorflow\n","import mlflow.keras\n","\n","# Auto-log all Keras models\n","mlflow.tensorflow.autolog()"]},{"cell_type":"markdown","metadata":{"id":"tlqpirCfr5Zq"},"source":["### Wrap Your Training in mlflow.start_run()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110612,"status":"aborted","timestamp":1761783116153,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"PavopbtoagPw"},"outputs":[],"source":["# === HYPERPARAMETERS (change these per experiment) ===\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3\n","DROPOUT_RATE = 0.3\n","EPOCHS = 50\n","\n","# === START MLflow RUN ===\n","with mlflow.start_run(run_name=f\"cnn_lr{LEARNING_RATE}_bs{BATCH_SIZE}_img{IMG_SIZE[0]}\") as run:\n","\n","    # Log hyperparameters\n","    mlflow.log_param(\"img_size\", IMG_SIZE[0])\n","    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n","    mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n","    mlflow.log_param(\"dropout_rate\", DROPOUT_RATE)\n","    mlflow.log_param(\"epochs\", EPOCHS)\n","    mlflow.log_param(\"model_type\", \"CustomCNN\")\n","    mlflow.log_param(\"augmentation\", \"full\")\n","\n","    # === PREPARE DATA GENERATORS ===\n","    # Recreate generators within this cell to ensure correct class_mode\n","    # Assumes 'prepare_dataset' function is defined in a previous cell and executed\n","    try:\n","        # Try to use existing 'path' if available, otherwise assume a default or download\n","        # This part might need adjustment based on how 'path' is truly managed\n","        dataset_root_path = path # Assuming 'path' is the root of the dataset\n","    except NameError:\n","        print(\"Dataset path 'path' not found. Assuming a default path or you need to define it.\")\n","        # Fallback or error handling if path is not defined\n","        # For now, let's assume 'path' exists from previous cells\n","        pass\n","\n","    # Call the prepare_dataset function to get the generators\n","    train_gen, val_gen, test_gen, df_full = prepare_dataset(\n","        root_path=dataset_root_path, # Use the dataset root path\n","        img_size=IMG_SIZE,\n","        batch_size=BATCH_SIZE\n","    )\n","\n","    # Ensure label2index and index2label are available from the generators\n","    label2index = train_gen.class_indices\n","    index2label = {v: k for k, v in label2index.items()}\n","    print(\"Label mapping (label -\u003e index):\", label2index)\n","\n","\n","    # === BUILD MODEL ===\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n","    from tensorflow.keras.optimizers import Adam\n","\n","    num_classes = len(label2index) # Get num_classes from the generator\n","\n","    model = Sequential([\n","        Conv2D(32, 3, activation='relu', input_shape=(*IMG_SIZE, 3)),\n","        BatchNormalization(),\n","        MaxPooling2D(2),\n","\n","        Conv2D(64, 3, activation='relu'),\n","        BatchNormalization(),\n","        MaxPooling2D(2),\n","\n","        Conv2D(128, 3, activation='relu'),\n","        BatchNormalization(),\n","        GlobalAveragePooling2D(),\n","\n","        Dense(128, activation='relu'),\n","        Dropout(DROPOUT_RATE),\n","        Dense(num_classes, activation='softmax') # Use num_classes from generator\n","    ])\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=LEARNING_RATE),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n","    )\n","\n","    # === CALLBACKS ===\n","    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","    callbacks = [\n","        EarlyStopping(patience=7, restore_best_weights=True, monitor='val_loss'),\n","        ReduceLROnPlateau(patience=3, factor=0.5, monitor='val_loss'),\n","        ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_accuracy')\n","    ]\n","\n","    # === TRAIN ===\n","    history = model.fit(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=EPOCHS,\n","        callbacks=callbacks,\n","        verbose=1\n","    )\n","\n","    # === LOG FINAL METRICS ===\n","    best_val_acc = max(history.history['val_accuracy'])\n","    # Check if 'val_auc' is in history before accessing it\n","    best_val_auc = max(history.history['val_auc']) if 'val_auc' in history.history else None\n","\n","    mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n","    if best_val_auc is not None:\n","      mlflow.log_metric(\"best_val_auc\", best_val_auc)\n","    mlflow.log_metric(\"final_train_loss\", history.history['loss'][-1])\n","\n","    # === LOG MODEL ===\n","    mlflow.keras.log_model(model, \"model\")\n","\n","    # === LOG CONFUSION MATRIX PLOT ===\n","    import matplotlib.pyplot as plt\n","    from sklearn.metrics import confusion_matrix\n","    import seaborn as sns\n","    import numpy as np\n","\n","    # Need to reset test_gen before predicting\n","    test_gen.reset()\n","    # Get true labels from the test generator\n","    y_true = test_gen.classes\n","    # Predict probabilities\n","    y_pred_probs = model.predict(test_gen)\n","    # Get predicted class indices\n","    y_pred = np.argmax(y_pred_probs, axis=1)\n","\n","\n","    cm = confusion_matrix(y_true, y_pred)\n","    plt.figure(figsize=(6,5))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=test_gen.class_indices.keys(),\n","                yticklabels=test_gen.class_indices.keys())\n","    plt.title(\"Confusion Matrix\")\n","    plt.ylabel(\"True\")\n","    plt.xlabel(\"Predicted\")\n","    plt.tight_layout()\n","    plt.savefig(\"confusion_matrix.png\")\n","    plt.close()\n","\n","    mlflow.log_artifact(\"confusion_matrix.png\")\n","    mlflow.log_artifact(\"best_model.h5\")\n","\n","    print(f\"MLflow Run ID: {run.info.run_id}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110611,"status":"aborted","timestamp":1761783116154,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"6084140f-6b54-42a4-8f12-0f1317862048"},"outputs":[],"source":["# Install ngrok by downloading the binary\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","!chmod +x ngrok\n","!sudo mv ngrok /usr/local/bin/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110609,"status":"aborted","timestamp":1761783116154,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"sXgd9LGnlPS5"},"outputs":[],"source":["!ngrok authtoken 34kPpTGbA8SPniLPhSFXRl3VjEE_4pT93d7LLqCaiq4j8Ub9m # replace with your own ngrok authtoken"]},{"cell_type":"markdown","metadata":{"id":"PFP2nHEssFoJ"},"source":["### 4. Launch MLflow UI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110608,"status":"aborted","timestamp":1761783116155,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"Fry3aEdrgc1I"},"outputs":[],"source":["# Kill any running ngrok processes to free up the address\n","!kill $(ps aux | grep 'ngrok' | grep -v 'grep' | awk '{print $2}') 2\u003e/dev/null || true\n","\n","\n","# Start MLflow UI in background\n","import subprocess\n","get_ipython().system_raw(\"mlflow ui --port 5000 \u0026\")\n","\n","# Create tunnel using ngrok (install first if needed)\n","!pip install pyngrok --quiet\n","\n","from pyngrok import ngrok\n","\n","# Set your ngrok authtoken for pyngrok\n","ngrok.set_auth_token(\"34kPpTGbA8SPniLPhSFXRl3VjEE_4pT93d7LLqCaiq4j8Ub9m\")\n","\n","\n","public_url = ngrok.connect(5000)\n","print(f\"MLflow UI: {public_url}\")"]},{"cell_type":"markdown","metadata":{"id":"doIzn3TFsO4W"},"source":["### Run Multiple Experiments (Hyperparameter Search)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110609,"status":"aborted","timestamp":1761783116156,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"CzQqgf7Ol1q_"},"outputs":[],"source":["# Experiment 1\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 32\n","# → Run the block above\n","\n","# Experiment 2\n","LEARNING_RATE = 5e-4\n","BATCH_SIZE = 16\n","# → Run again\n","\n","# Experiment 3\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 32\n","# → Run again"]},{"cell_type":"markdown","metadata":{"id":"pwJqZar-sSE6"},"source":["### Add Hyperparameter Tuning with Keras Tuner\n","1. Install Keras Tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110643,"status":"aborted","timestamp":1761783116191,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"e_c3pBdGmZOy"},"outputs":[],"source":["!pip install keras-tuner --quiet"]},{"cell_type":"markdown","metadata":{"id":"lmJf7vw8srFw"},"source":["### 2. Define the Model Builder Function (Hypermodel)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110644,"status":"aborted","timestamp":1761783116192,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"jae9q4PnmhY4"},"outputs":[],"source":["import keras_tuner as kt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","\n","def build_model(hp):\n","    model = Sequential([\n","        Conv2D(\n","            filters=32,\n","            kernel_size=3,\n","            activation='relu',\n","            input_shape=(224, 224, 3)\n","        ),\n","        BatchNormalization(),\n","        MaxPooling2D(2),\n","\n","        Conv2D(\n","            filters=64,\n","            kernel_size=3,\n","            activation='relu'\n","        ),\n","        BatchNormalization(),\n","        MaxPooling2D(2),\n","\n","        Conv2D(\n","            filters=128,\n","            kernel_size=3,\n","            activation='relu'\n","        ),\n","        BatchNormalization(),\n","        GlobalAveragePooling2D(),\n","\n","        Dense(128, activation='relu'),\n","        Dropout(\n","            rate=hp.Float('dropout', min_value=0.3, max_value=0.5, step=0.1)\n","        ),\n","        Dense(2, activation='softmax')\n","    ])\n","\n","    # Tune learning rate\n","    lr = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n","    model.compile(\n","        optimizer=Adam(learning_rate=lr),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n","    )\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"kKt0NPq4syVp"},"source":["### Set Up Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110644,"status":"aborted","timestamp":1761783116193,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"rTP3f46JmmT6"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","callbacks = [\n","    EarlyStopping(patience=7, restore_best_weights=True, monitor='val_loss'),\n","    ReduceLROnPlateau(patience=3, factor=0.5, monitor='val_loss'),\n","    ModelCheckpoint(\"best_model_tuner.h5\", save_best_only=True, monitor='val_accuracy')\n","]"]},{"cell_type":"markdown","metadata":{"id":"e-lxVDTls4I7"},"source":["### Run Hyperparameter Search with MLflow Tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110644,"status":"aborted","timestamp":1761783116194,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"V7GxLxKcmqtq"},"outputs":[],"source":["import mlflow\n","import mlflow.keras\n","\n","# Enable autologging\n","mlflow.keras.autolog()\n","\n","# Define search space\n","tuner = kt.RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=6,  # 2 batch sizes × 3 LRs = 6 combos\n","    executions_per_trial=1,\n","    directory='tuner_dir',\n","    project_name='conjunctivitis_tuning'\n",")\n","\n","# Start MLflow run for the *entire search*\n","with mlflow.start_run(run_name=\"Hyperparameter_Search_RandomSearch\") as tuning_run:\n","    mlflow.log_param(\"tuner_type\", \"RandomSearch\")\n","    mlflow.log_param(\"max_trials\", 6)\n","    mlflow.log_param(\"objective\", \"val_accuracy\")\n","\n","    print(\"Starting hyperparameter search...\")\n","    tuner.search(\n","        train_gen,\n","        validation_data=val_gen,\n","        epochs=50,\n","        callbacks=callbacks,\n","        verbose=1\n","    )\n","\n","    # Get best model\n","    best_model = tuner.get_best_models(num_models=1)[0]\n","    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","    # Log best hyperparameters\n","    mlflow.log_params({\n","        \"best_learning_rate\": best_hyperparameters.get('learning_rate'),\n","        \"best_dropout\": best_hyperparameters.get('dropout'),\n","        \"best_batch_size\": train_gen.batch_size  # We'll set this below\n","    })\n","\n","    # Evaluate on test set\n","    test_loss, test_acc, test_auc, test_precision, test_recall = best_model.evaluate(test_gen, verbose=0)\n","    mlflow.log_metrics({\n","        \"test_accuracy\": test_acc,\n","        \"test_auc\": test_auc,\n","        \"test_precision\": test_precision,\n","        \"test_recall\": test_recall\n","    })\n","\n","    # Save best model\n","    best_model.save(\"best_tuned_model.h5\")\n","    mlflow.log_artifact(\"best_tuned_model.h5\")\n","\n","    print(f\"\\nBest val accuracy: {tuner.oracle.get_best_trials(1)[0].score:.4f}\")\n","    print(f\"Best hyperparameters: {best_hyperparameters.values}\")"]},{"cell_type":"markdown","metadata":{"id":"fecxw03ZtY0E"},"source":["Handle Batch Size in Search (Keras Tuner Can’t Change batch_size in .fit())\n","We loop over batch sizes manually and let Keras Tuner tune the rest:"]},{"cell_type":"markdown","metadata":{"id":"RKazu0d9tgRq"},"source":["### 6. View Results in MLflow UI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110643,"status":"aborted","timestamp":1761783116194,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"LbiF0yUUm348"},"outputs":[],"source":["# Launch UI (run once)\n","import subprocess\n","get_ipython().system_raw(\"mlflow ui --port 5000 \u0026\")\n","from pyngrok import ngrok\n","print(\"MLflow UI:\", ngrok.connect(5000))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110642,"status":"aborted","timestamp":1761783116195,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"luGDgPRRmynw"},"outputs":[],"source":["# === FULL HYPERPARAMETER SEARCH WITH BATCH SIZE LOOP ===\n","import mlflow\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator # Import ImageDataGenerator\n","import keras_tuner as kt # Import keras_tuner here\n","\n","batch_sizes = [16, 32]\n","\n","# Define ImageDataGenerator objects globally for use within the loop\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","val_datagen = ImageDataGenerator(rescale=1./255) # Use a separate object for validation/test\n","\n","with mlflow.start_run(run_name=\"Full_Hyperparameter_Search\") as parent_run:\n","    best_overall_score = 0\n","    best_model_overall = None # Renamed to avoid conflict\n","    best_hps_overall = None   # Renamed to avoid conflict\n","    best_batch_size_overall = None # Renamed to avoid conflict\n","\n","    # Assume train_df, val_df, test_df are defined in a previous cell\n","    try:\n","        train_df\n","        val_df\n","        test_df\n","    except NameError:\n","        print(\"DataFrames (train_df, val_df, test_df) not found. Please ensure they are created before running this cell.\")\n","        # Exit or handle error appropriately if dataframes are missing\n","        # For now, we'll assume they exist from previous steps\n","\n","    # === HYPERPARAMETERS (change these per experiment) ===\n","    IMG_SIZE = (128, 128) # Reduced image size for faster tuning\n","    # BATCH_SIZE = 32 # This will be overridden by the loop\n","    # LEARNING_RATE = 1e-3 # This will be tuned by Keras Tuner\n","    # DROPOUT_RATE = 0.3 # This will be tuned by Keras Tuner\n","    EPOCHS = 50 # You might also reduce epochs for faster tuning\n","\n","    for batch_size in batch_sizes:\n","        print(f\"\\n=== Tuning with batch_size = {batch_size} ===\")\n","\n","        # Recreate data generators with new batch size and smaller image size\n","        # Use the globally defined datagen objects\n","        train_gen_bs = train_datagen.flow_from_dataframe(\n","            train_df,\n","            x_col='image_path',\n","            y_col='label',\n","            target_size=IMG_SIZE, # Use the reduced IMG_SIZE\n","            batch_size=batch_size,\n","            class_mode='categorical',\n","            shuffle=True\n","        )\n","        val_gen_bs = val_datagen.flow_from_dataframe(\n","            val_df,\n","            x_col='image_path',\n","            y_col='label',\n","            target_size=IMG_SIZE, # Use the reduced IMG_SIZE\n","            batch_size=batch_size,\n","            class_mode='categorical',\n","            shuffle=False\n","        )\n","        # Also create test_gen_bs for evaluation later\n","        test_gen_bs = val_datagen.flow_from_dataframe(\n","            test_df,\n","            x_col='image_path',\n","            y_col='label',\n","            target_size=IMG_SIZE, # Use the reduced IMG_SIZE\n","            batch_size=batch_size, # Use the current batch_size for test as well\n","            class_mode='categorical',\n","            shuffle=False\n","        )\n","\n","\n","        # New tuner per batch size\n","        tuner = kt.RandomSearch(\n","            build_model, # build_model function must be defined in a previous cell\n","            objective='val_accuracy',\n","            max_trials=3,  # 3 LRs × 2 dropouts = 6, but we limit to 3 trials per batch size\n","            executions_per_trial=1,\n","            directory=f'tuner_dir_bs{batch_size}',\n","            project_name='conjunctivitis_tuning_bs' # Unique project name per batch size to avoid conflicts\n","        )\n","\n","        with mlflow.start_run(nested=True, run_name=f\"batch_size_{batch_size}\") as child_run:\n","            mlflow.log_param(\"batch_size\", batch_size)\n","            mlflow.log_param(\"img_size\", IMG_SIZE[0]) # Log the image size for this run\n","\n","\n","            tuner.search(\n","                train_gen_bs,\n","                validation_data=val_gen_bs,\n","                epochs=EPOCHS,\n","                callbacks=callbacks, # callbacks list must be defined in a previous cell\n","                verbose=0\n","            )\n","\n","            # Get the best trial and its score for this batch size\n","            best_trial_for_bs = tuner.oracle.get_best_trials(1)[0]\n","            score_for_bs = best_trial_for_bs.score\n","            hps_for_bs = best_trial_for_bs.hyperparameters.values # Keep this line as it correctly gets the dict\n","\n","            mlflow.log_metric(\"val_accuracy_for_batch_size\", score_for_bs)\n","            # Log hyperparameters for this batch size run (Keras Tuner autologging should handle most)\n","            # mlflow.log_params(hps_for_bs) # Removed to avoid conflict with autologging\n","\n","\n","            # Check if this batch size's best score is the overall best\n","            if score_for_bs \u003e best_overall_score:\n","                best_overall_score = score_for_bs\n","                # Get the best model and hyperparameters from the tuner\n","                best_model_overall = tuner.get_best_models(1)[0]\n","                best_hps_overall = tuner.get_best_hyperparameters(1)[0].values # Get as dict\n","                best_batch_size_overall = batch_size\n","\n","    # === LOG BEST OVERALL ===\n","    mlflow.log_metric(\"best_overall_val_accuracy\", best_overall_score)\n","    mlflow.log_param(\"best_overall_batch_size\", best_batch_size_overall)\n","    if best_hps_overall:\n","        mlflow.log_params(best_hps_overall)\n","\n","    # Evaluate the best overall model on the test set\n","    # Need to ensure test_gen corresponds to the best_batch_size\n","    # Recreate test_gen one last time with the best overall batch size\n","    if best_batch_size_overall is not None:\n","        final_test_gen = val_datagen.flow_from_dataframe(\n","            test_df,\n","            x_col='image_path',\n","            y_col='label',\n","            target_size=IMG_SIZE, # Use the reduced IMG_SIZE\n","            batch_size=best_batch_size_overall,\n","            class_mode='categorical',\n","            shuffle=False\n","        )\n","    else:\n","         # Handle case where no trials were successful\n","         final_test_gen = None\n","\n","\n","    # Need to ensure best_model_overall is not None if no trials ran\n","    if best_model_overall is not None and final_test_gen is not None:\n","        # Unpack all metrics returned by evaluate()\n","        test_loss, test_acc, test_auc, test_precision, test_recall = best_model_overall.evaluate(final_test_gen, verbose=0)\n","        mlflow.log_metric(\"final_test_accuracy\", test_acc)\n","        mlflow.log_metric(\"final_test_loss\", test_loss) # Log test loss too\n","        mlflow.log_metric(\"final_test_auc\", test_auc)\n","        mlflow.log_metric(\"final_test_precision\", test_precision)\n","        mlflow.log_metric(\"final_test_recall\", test_recall)\n","\n","\n","        # Save best model\n","        best_model_overall.save(\"final_best_model.h5\")\n","        mlflow.log_artifact(\"final_best_model.h5\")\n","    else:\n","        print(\"No successful tuning trials or test generator to log best model results.\")\n","\n","\n","    print(f\"\\nBEST OVERALL MODEL: batch_size={best_batch_size_overall}, val_acc={best_overall_score:.4f}\")\n","    if best_hps_overall:\n","        print(f\"Hyperparameters: {best_hps_overall}\")"]},{"cell_type":"markdown","metadata":{"id":"kiGtl4g9yMo7"},"source":["###Install Extra Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110642,"status":"aborted","timestamp":1761783116195,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"fTyZenlcySA7"},"outputs":[],"source":["!pip install scikit-learn matplotlib seaborn --quiet"]},{"cell_type":"markdown","metadata":{"id":"_E4GReccyXXC"},"source":["### 2. Load Your Best Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110641,"status":"aborted","timestamp":1761783116196,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"ISxmTghCyZi5"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Replace with your actual path\n","best_model = load_model(\"final_best_model.h5\")"]},{"cell_type":"markdown","metadata":{"id":"T5DdEGaPyqAV"},"source":["### 3. Full Evaluation with All Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110639,"status":"aborted","timestamp":1761783116196,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"PhWM8iD4yvuC"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import (\n","    roc_curve, auc, precision_recall_curve, average_precision_score,\n","    confusion_matrix, classification_report, cohen_kappa_score\n",")\n","import mlflow\n","\n","# Start MLflow run for evaluation\n","with mlflow.start_run(run_name=\"Full_Metric_Evaluation\") as eval_run:\n","\n","    # === PREDICTIONS ===\n","    y_pred_prob = best_model.predict(test_gen, verbose=0)\n","    y_pred = np.argmax(y_pred_prob, axis=1)\n","    y_true = test_gen.classes\n","\n","    # Get class labels\n","    labels = list(test_gen.class_indices.keys())  # ['healthy_eye', 'infected_eye']\n","\n","    # === 1. ROC-AUC (One-vs-Rest) ===\n","    from sklearn.preprocessing import label_binarize\n","    y_true_bin = label_binarize(y_true, classes=[0, 1])\n","    fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_pred_prob[:, 1])\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure(figsize=(6,5))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(alpha=0.3)\n","    plt.savefig(\"roc_curve.png\")\n","    plt.close()\n","    mlflow.log_artifact(\"roc_curve.png\")\n","\n","    # === 2. PR-AUC ===\n","    precision, recall, _ = precision_recall_curve(y_true_bin.ravel(), y_pred_prob[:, 1])\n","    pr_auc = average_precision_score(y_true_bin, y_pred_prob[:, 1])\n","\n","    plt.figure(figsize=(6,5))\n","    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.3f})')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Precision-Recall Curve')\n","    plt.legend(loc=\"upper right\")\n","    plt.grid(alpha=0.3)\n","    plt.savefig(\"pr_curve.png\")\n","    plt.close()\n","    mlflow.log_artifact(\"pr_curve.png\")\n","\n","    # === 3. Confusion Matrix + Per-class Metrics ===\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","\n","    sensitivity = tp / (tp + fn)  # Recall for infected\n","    specificity = tn / (tn + fp)  # True Negative Rate for healthy\n","    precision_pos = tp / (tp + fp)\n","    f1_pos = 2 * precision_pos * sensitivity / (precision_pos + sensitivity)\n","\n","    plt.figure(figsize=(6,5))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=labels, yticklabels=labels)\n","    plt.title(\"Confusion Matrix\")\n","    plt.ylabel(\"True Label\")\n","    plt.xlabel(\"Predicted Label\")\n","    plt.savefig(\"confusion_matrix_eval.png\")\n","    plt.close()\n","    mlflow.log_artifact(\"confusion_matrix_eval.png\")\n","\n","    # === 4. Cohen’s Kappa ===\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    # === 5. Full Classification Report ===\n","    report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n","    acc = report['accuracy']\n","\n","    # === LOG ALL METRICS ===\n","    mlflow.log_metric(\"test_accuracy\", acc)\n","    mlflow.log_metric(\"roc_auc\", roc_auc)\n","    mlflow.log_metric(\"pr_auc\", pr_auc)\n","    mlflow.log_metric(\"sensitivity_infected\", sensitivity)\n","    mlflow.log_metric(\"specificity_healthy\", specificity)\n","    mlflow.log_metric(\"precision_infected\", precision_pos)\n","    mlflow.log_metric(\"f1_infected\", f1_pos)\n","    mlflow.log_metric(\"cohen_kappa\", kappa)\n","\n","    # === PRINT SUMMARY ===\n","    print(\"FULL METRIC SUITE\")\n","    print(f\"Accuracy:          {acc:.4f}\")\n","    print(f\"ROC-AUC:           {roc_auc:.4f}\")\n","    print(f\"PR-AUC (AP):       {pr_auc:.4f}\")\n","    print(f\"Sensitivity (Inf): {sensitivity:.4f}\")\n","    print(f\"Specificity (Hlt): {specificity:.4f}\")\n","    print(f\"Precision (Inf):   {precision_pos:.4f}\")\n","    print(f\"F1-Score (Inf):    {f1_pos:.4f}\")\n","    print(f\"Cohen's Kappa:     {kappa:.4f}\")\n","    print(\"\\nConfusion Matrix:\")\n","    print(cm)\n","    print(f\"\\nMLflow Run ID: {eval_run.info.run_id}\")"]},{"cell_type":"markdown","metadata":{"id":"iTUUH9R3ziaP"},"source":["### 2. Full Error Analysis Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110638,"status":"aborted","timestamp":1761783116196,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"bGEc8XXozl_N"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cv2\n","import pandas as pd\n","from tensorflow.keras.models import load_model\n","import mlflow\n","\n","# Load best model\n","best_model = load_model(\"final_best_model.h5\")\n","\n","# Start MLflow run\n","with mlflow.start_run(run_name=\"Error_Analysis\") as error_run:\n","\n","    # === 1. Get predictions on test set ===\n","    y_pred_prob = best_model.predict(test_gen, verbose=0)\n","    y_pred = np.argmax(y_pred_prob, axis=1)\n","    y_true = test_gen.classes\n","    filenames = test_gen.filenames  # relative paths\n","\n","    # Map filenames to full paths from test_df\n","    path_map = dict(zip(test_df['image_path'].apply(lambda x: x.split('/')[-1]), test_df['image_path']))\n","    full_paths = [path_map.get(f.split('/')[-1], f) for f in filenames]\n","\n","    # === 2. Find misclassified indices ===\n","    misclassified_idx = np.where(y_pred != y_true)[0]\n","    print(f\"Found {len(misclassified_idx)} misclassified images.\")\n","\n","    # === 3. Plot Misclassified Images (2x3 Grid) ===\n","    n_plot = min(6, len(misclassified_idx))\n","    if n_plot \u003e 0:\n","        fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n","        axes = axes.ravel()\n","\n","        for i in range(n_plot):\n","            idx = misclassified_idx[i]\n","            img_path = full_paths[idx]\n","            true_label = list(test_gen.class_indices.keys())[y_true[idx]]\n","            pred_label = list(test_gen.class_indices.keys())[y_pred[idx]]\n","            confidence = y_pred_prob[idx].max()\n","\n","            # Load and preprocess image\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            img_resized = cv2.resize(img, (224, 224))\n","\n","            axes[i].imshow(img_resized)\n","            axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}\",\n","                              fontsize=10, color='red' if true_label != pred_label else 'green')\n","            axes[i].axis('off')\n","\n","        # Hide empty subplots\n","        for j in range(n_plot, 6):\n","            axes[j].axis('off')\n","\n","        plt.suptitle(\"Misclassified Images (Error Analysis)\", fontsize=14)\n","        plt.tight_layout()\n","        plt.savefig(\"misclassified_grid.png\")\n","        plt.close()\n","\n","        mlflow.log_artifact(\"misclassified_grid.png\")\n","    else:\n","        print(\"No misclassified images to display.\")\n","\n","    # === 4. Brightness \u0026 Contrast Analysis ===\n","    brightness = []\n","    contrast = []\n","    error_type = []\n","\n","    for idx in range(len(y_true)):\n","        img_path = full_paths[idx]\n","        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, (224, 224))\n","\n","        # Brightness = mean pixel intensity\n","        bright = np.mean(img)\n","\n","        # Contrast = standard deviation\n","        contr = np.std(img)\n","\n","        true_label = list(test_gen.class_indices.keys())[y_true[idx]]\n","        pred_label = list(test_gen.class_indices.keys())[y_pred[idx]]\n","\n","        brightness.append(bright)\n","        contrast.append(contr)\n","        error_type.append(\"Correct\" if true_label == pred_label else \"Misclassified\")\n","\n","    # Create DataFrame\n","    error_df = pd.DataFrame({\n","        'brightness': brightness,\n","        'contrast': contrast,\n","        'error': error_type\n","    })\n","\n","    # === 5. Heatmap: Errors vs Brightness/Contrast ===\n","    # Bin into 5x5 grid\n","    error_df['bright_bin'] = pd.cut(error_df['brightness'], bins=5, labels=False)\n","    error_df['contrast_bin'] = pd.cut(error_df['contrast'], bins=5, labels=False)\n","\n","    # Count errors per bin\n","    heatmap_data = error_df[error_df['error'] == 'Misclassified'] \\\n","        .groupby(['bright_bin', 'contrast_bin']).size().unstack(fill_value=0)\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Reds', cbar_kws={'label': 'Misclassification Count'})\n","    plt.title(\"Misclassifications by Brightness \u0026 Contrast\")\n","    plt.xlabel(\"Contrast Bin (Low to High)\")\n","    plt.ylabel(\"Brightness Bin (Low to High)\")\n","    plt.tight_layout()\n","    plt.savefig(\"error_heatmap_brightness_contrast.png\")\n","    plt.close()\n","\n","    mlflow.log_artifact(\"error_heatmap_brightness_contrast.png\")\n","\n","    # === 6. Save error summary ===\n","    total_errors = len(misclassified_idx)\n","    error_rate = total_errors / len(y_true)\n","    mlflow.log_metric(\"misclassification_count\", total_errors)\n","    mlflow.log_metric(\"error_rate\", error_rate)\n","\n","    # Optional: save error_df\n","    error_df.to_csv(\"error_analysis_data.csv\", index=False)\n","    mlflow.log_artifact(\"error_analysis_data.csv\")\n","\n","    print(f\"\\nError Analysis Complete:\")\n","    print(f\"  • Misclassified: {total_errors}/{len(y_true)} ({error_rate:.2%})\")\n","    print(f\"  • Grid plot: misclassified_grid.png\")\n","    print(f\"  • Heatmap: error_heatmap_brightness_contrast.png\")\n","    print(f\"  • MLflow Run ID: {error_run.info.run_id}\")"]},{"cell_type":"markdown","metadata":{"id":"vT_tWRy6zufz"},"source":["MLflow UI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110638,"status":"aborted","timestamp":1761783116197,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"reWMcXd9zwxN"},"outputs":[],"source":["import subprocess\n","get_ipython().system_raw(\"mlflow ui --port 5000 \u0026\")\n","from pyngrok import ngrok\n","print(\"MLflow UI:\", ngrok.connect(5000))"]},{"cell_type":"markdown","metadata":{"id":"IkuIjYXV0DdN"},"source":["Auto Generate Comparison table"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":110636,"status":"aborted","timestamp":1761783116197,"user":{"displayName":"Javon Darby","userId":"05512817070720551351"},"user_tz":300},"id":"xE_66rve0HcP"},"outputs":[],"source":["import mlflow\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","# Start MLflow client\n","client = mlflow.tracking.MlflowClient()\n","\n","# === 1. Get all runs from your project ===\n","experiment_name = \"Default\"  # Change if you set a custom name\n","try:\n","    exp = client.get_experiment_by_name(experiment_name)\n","    experiment_id = exp.experiment_id\n","except:\n","    experiment_id = \"0\"  # default\n","\n","runs = client.search_runs(\n","    experiment_ids=[experiment_id],\n","    filter_string=\"\",\n","    run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY\n",")\n","\n","# === 2. Extract data ===\n","data = []\n","for run in runs:\n","    params = run.data.params\n","    metrics = run.data.metrics\n","\n","    # Only include runs with test evaluation\n","    if 'test_accuracy' not in metrics:\n","        continue\n","\n","    data.append({\n","        'Model ID': run.info.run_id[:8],\n","        'Run Name': run.data.tags.get('mlflow.runName', 'unnamed'),\n","        'Learning Rate': float(params.get('learning_rate', params.get('lr', 0))),\n","        'Batch Size': int(params.get('batch_size', 0)),\n","        'Dropout': float(params.get('dropout', 0)),\n","        'Val Accuracy': round(metrics.get('val_accuracy', metrics.get('best_val_accuracy', 0)), 4),\n","        'Test Accuracy': round(metrics.get('test_accuracy', 0), 4),\n","        'ROC-AUC': round(metrics.get('roc_auc', 0), 4),\n","        'PR-AUC': round(metrics.get('pr_auc', 0), 4),\n","        'Kappa': round(metrics.get('cohen_kappa', 0), 4),\n","        'Params (K)': round(int(params.get('trainable_params', 94000)) / 1000, 1),\n","        'Date': datetime.fromtimestamp(run.info.start_time / 1000).strftime('%m-%d %H:%M')\n","    })\n","\n","# === 3. Add your baseline (prototype) model from notebook ===\n","data.append({\n","    'Model ID': 'baseline',\n","    'Run Name': 'Prototype (128×128)',\n","    'Learning Rate': 0.001,\n","    'Batch Size': 32,\n","    'Dropout': 0.0,\n","    'Val Accuracy': 0.9300,  # from your training log\n","    'Test Accuracy': 0.9307,  # from evaluation\n","    'ROC-AUC': 0.987,        # from classification report\n","    'PR-AUC': 0.987,\n","    'Kappa': 0.861,          # estimated from report\n","    'Params (K)': 94.0,\n","    'Date': '10-28 14:00'\n","})\n","\n","# Sort by Test Accuracy\n","df = pd.DataFrame(data)\n","df = df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n","\n","# === 4. Create Markdown Table ===\n","def make_comparison_md(df):\n","    md = \"\"\"# Model Comparison Table\\n\\n\"\"\"\n","    md += \"\u003e **Week 9 Deliverable** – Comparison of all trained models\\n\\n\"\n","    md += df.to_markdown(index=False)\n","    md += \"\\n\\n---\\n*Best model highlighted in bold*\\n\"\n","    return md\n","\n","comparison_md = make_comparison_md(df)\n","\n","# Print\n","print(comparison_md)\n","\n","# Save\n","with open(\"comparison_table.md\", \"w\") as f:\n","    f.write(comparison_md)\n","\n","# === 5. Log to MLflow ===\n","with mlflow.start_run(run_name=\"Model_Comparison_Table\") as compare_run:\n","    mlflow.log_artifact(\"comparison_table.md\")\n","    mlflow.log_text(comparison_md, \"comparison_table.txt\")\n","    mlflow.log_metric(\"n_models_compared\", len(df))\n","    print(f\"Comparison table logged! Run ID: {compare_run.info.run_id}\")"]},{"cell_type":"markdown","metadata":{"id":"26e099f8"},"source":["# Model Comparison Summary\n","\n","This table compares the performance of different models trained on the Conjunctivitis Dataset, based on metrics logged to MLflow.\n","\n","| Column          | Description                                                                 | What it tells you                                                                 |\n","| :-------------- | :-------------------------------------------------------------------------- | :-------------------------------------------------------------------------------- |\n","| **Model ID**    | A unique identifier for the MLflow run (or a label like 'baseline').        | Helps you distinguish between different experiments.                              |\n","| **Run Name**    | A human-readable name given to the MLflow run.                              | Provides context for the experiment (e.g., 'Prototype', 'Hyperparameter\\_Search'). |\n","| **Learning Rate** | The learning rate used during training.                                     | A key hyperparameter affecting how quickly the model learns.                      |\n","| **Batch Size**  | The number of samples processed before updating the model weights.          | Another key hyperparameter affecting training stability and speed.                  |\n","| **Dropout**     | The dropout rate used in the model (a regularization technique).            | Helps prevent overfitting.                                                        |\n","| **Val Accuracy**| The accuracy of the model on the validation set during training.            | Indicates how well the model generalizes to unseen data during training.          |\n","| **Test Accuracy**| The accuracy of the model on the held-out test set after training.          | The most important metric for evaluating the final performance on truly unseen data.|\n","| **ROC-AUC**     | Area Under the Receiver Operating Characteristic curve.                       | Measures the model's ability to distinguish between classes (higher is better).   |\n","| **PR-AUC**      | Area Under the Precision-Recall curve (also called Average Precision - AP). | Useful for imbalanced datasets; measures the trade-off between precision and recall.|\n","| **Kappa**       | Cohen's Kappa Score.                                                        | Measures the agreement between the predicted and true labels, correcting for chance agreement.|\n","| **Params (K)**  | The number of trainable parameters in the model (in thousands).             | Indicates the model's complexity.                                                 |\n","| **Date**        | The date and time the experiment run started.                               | Helps track the timeline of your experiments.                                     |\n","\n","## Best Model Performance\n","\n","Based on the `Test Accuracy` metric in the comparison table, the best performing model was the **Prototype (128x128)** model, identified with `Model ID: baseline`.\n","\n","Its key performance metrics on the test set were:\n","\n","*   **Test Accuracy:** 0.9307\n","*   **ROC-AUC:** 0.9870\n","*   **PR-AUC (AP):** 0.9870\n","*   **Cohen's Kappa:** 0.8610\n","\n","This indicates that the initial prototype model achieved strong results in classifying healthy and infected eyes on the test dataset. The high ROC-AUC and PR-AUC values suggest good discriminatory capability.\n","\n","It's worth noting that the hyperparameter search runs logged to MLflow did not achieve comparable test performance in this table. Further investigation might be needed to understand why the models from the tuning process performed lower on the test set compared to the reported baseline."]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}